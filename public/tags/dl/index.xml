<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DL on Maxime Chemin</title>
    <link>http://localhost:1313/tags/dl/</link>
    <description>Recent content in DL on Maxime Chemin</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Automated Image Captioning (Bachelor Thesis)</title>
      <link>http://localhost:1313/projects/automated-image-captioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/automated-image-captioning/</guid>
      <description>&lt;h3 id=&#34;-colab-notebookhttpscolabresearchgooglecomdrive1q553uslyw3ho6p1g46soedxos_vmhxfj&#34;&gt;ðŸ”— &lt;a href=&#34;https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ&#34;&gt;Colab Notebook&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;&#xA;&lt;p&gt;In this project, I implemented the paper &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/1502.03044&#34;&gt;Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/a&gt;&lt;/strong&gt;. The neural network, a combination of &lt;strong&gt;CNN&lt;/strong&gt; and &lt;strong&gt;LSTM&lt;/strong&gt;, was trained on the &lt;strong&gt;MS COCO&lt;/strong&gt; dataset and it learns to generate captions from images.&lt;/p&gt;&#xA;&lt;p&gt;As the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.&#xA;&lt;img src=&#34;http://localhost:1313/projects/automated-image-captioning/img1.jpg&#34; alt=&#34;Attention Mechanism&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Face Landmarks Detection using CNN</title>
      <link>http://localhost:1313/projects/face-landmarks-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/face-landmarks-detection/</guid>
      <description>&lt;h3 id=&#34;-colab-notebookhttpscolabresearchgooglecomdrive1tow7w_wu4oltogzfz_0krpxmhdfr2gmb&#34;&gt;ðŸ”— &lt;a href=&#34;https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb&#34;&gt;Colab Notebook&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;-blog-postblogface-landmarks-detection&#34;&gt;ðŸ”— &lt;a href=&#34;../../blog/face-landmarks-detection&#34;&gt;Blog Post&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;&#xA;&lt;p&gt;In this project, I trained a neural network to localize key points on faces. &lt;strong&gt;Resnet-18&lt;/strong&gt; was used as the model with some slight modifications to the input and output layer. The model was trained on the official &lt;strong&gt;DLib Dataset&lt;/strong&gt; containing &lt;strong&gt;6666 images&lt;/strong&gt; along with corresponding &lt;strong&gt;68-point landmarks&lt;/strong&gt; for each face. Additionally, I wrote a custom data preprocessing pipeline in &lt;strong&gt;PyTorch&lt;/strong&gt; to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gaze-tracking Goggles</title>
      <link>http://localhost:1313/projects/gaze-tracking-goggles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/gaze-tracking-goggles/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;&#xA;&lt;p&gt;The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. &lt;strong&gt;Single Shot Descriptor&lt;/strong&gt;, with &lt;strong&gt;VGG16&lt;/strong&gt; as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using &lt;strong&gt;TkInter&lt;/strong&gt; for ease of use.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SEBART-Pro</title>
      <link>http://localhost:1313/projects/sebart-pro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/sebart-pro/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;&#xA;&lt;p&gt;I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. &lt;strong&gt;SEBART-Pro&lt;/strong&gt; is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an &lt;strong&gt;Arduino Nano&lt;/strong&gt; as the microcontroller. The robot senses the tilt using an &lt;strong&gt;MPU-6050 (6-axis gyroscope and accelerometer)&lt;/strong&gt; and converts the values from these sensors into angles using a &lt;strong&gt;Kalman Filter&lt;/strong&gt;. It uses the &lt;strong&gt;PID control algorithm&lt;/strong&gt; to balance on two wheels and a simple &lt;strong&gt;Convolutional Neural Network&lt;/strong&gt; is used to recognize traffic signs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
